{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b73a7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "import importlib\n",
    "\n",
    "import news_preprocessor\n",
    "import sentiment_analysis\n",
    "import news_feature_extractor\n",
    "import event_detector\n",
    "import news_visualizer\n",
    "import news_integration\n",
    "\n",
    "importlib.reload(news_preprocessor)\n",
    "importlib.reload(sentiment_analysis)\n",
    "importlib.reload(news_feature_extractor)\n",
    "importlib.reload(event_detector)\n",
    "importlib.reload(news_visualizer)\n",
    "importlib.reload(news_integration)\n",
    "\n",
    "from news_preprocessor import NewsPreprocessor\n",
    "from sentiment_analysis import SentimentAnalyzer\n",
    "from news_feature_extractor import NewsFeatureExtractor\n",
    "from event_detector import EventDetector\n",
    "from news_visualizer import NewsVisualizer\n",
    "from news_integration import NewsIntegration\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "699804c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/aeshef/Documents/GitHub/kursach'\n",
    "\n",
    "tickers = [\n",
    "    'GAZP', 'SBER'\n",
    "]\n",
    "\n",
    "preprocessor = NewsPreprocessor(base_dir)\n",
    "sentiment_analyzer = SentimentAnalyzer(language='russian')\n",
    "feature_extractor = NewsFeatureExtractor()\n",
    "event_detector = EventDetector()\n",
    "visualizer = NewsVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f1714a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПРЕДОБРАБОТКА НОВОСТЕЙ ===\n",
      "\n",
      "Предобработка новостей для тикера GAZP...\n",
      "/Users/aeshef/Documents/GitHub/kursach/data/processed_data/GAZP/news_analysis\n",
      "Объединенные данные сохранены в /Users/aeshef/Documents/GitHub/kursach/data/processed_data/GAZP/GAZP_all_news_processed.csv\n",
      "Обработано 139 новостей для GAZP\n",
      "Пример обработанной новости:\n",
      "Дата: 2025-01-03 00:00:00\n",
      "Текст до: #Политика #SIBN #GAZP\n",
      "Вучич заявил об отсутствии «хороших новостей» после разговора с США по NIS - К...\n",
      "Текст после: Политика SIBN GAZP Вучич заявил об отсутствии хороших новостей после разговора с США по NIS - Коммерсант Господин Вучич сообщал 14 декабря, что США могут ввести санкции против NIS, так как совладельцами компании являются Газпром нефть (50) и Газпром (6,15). 16 декабря президент Сербии объявлял о начале переговоров с США и Россией из-за возможных ограничений[:100]...\n",
      "\n",
      "Предобработка новостей для тикера SBER...\n",
      "/Users/aeshef/Documents/GitHub/kursach/data/processed_data/SBER/news_analysis\n",
      "Объединенные данные сохранены в /Users/aeshef/Documents/GitHub/kursach/data/processed_data/SBER/SBER_all_news_processed.csv\n",
      "Обработано 43 новостей для SBER\n",
      "Пример обработанной новости:\n",
      "Дата: 2025-01-01 00:00:00\n",
      "Текст до: #SBER\n",
      "**Правительству РФ и Сберу поручено обеспечить сотрудничество с Китаем при проведении исследов...\n",
      "Текст после: SBER Правительству РФ и Сберу поручено обеспечить сотрудничество с Китаем при проведении исследований в сфере ИИ Интерфакс[:100]...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ПРЕДОБРАБОТКА НОВОСТЕЙ ===\")\n",
    "processed_news = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"\\nПредобработка новостей для тикера {ticker}...\")\n",
    "    \n",
    "    output_dir = os.path.join(base_dir, 'data', 'processed_data', ticker, 'news_analysis')\n",
    "    print(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    news_df = preprocessor.process_ticker_news(ticker, save=True)\n",
    "    \n",
    "    if not news_df.empty:\n",
    "        processed_news[ticker] = news_df\n",
    "        print(f\"Обработано {len(news_df)} новостей для {ticker}\")\n",
    "        \n",
    "        if len(news_df) > 0:\n",
    "            print(\"Пример обработанной новости:\")\n",
    "            first_news = news_df.iloc[0]\n",
    "            print(f\"Дата: {first_news.get('date', 'Нет даты')}\")\n",
    "            print(f\"Текст до: {first_news.get('text', '')[:100]}...\")\n",
    "            print(f\"Текст после: {first_news.get('clean_text', '')}[:100]...\")\n",
    "    else:\n",
    "        print(f\"Нет новостей для тикера {ticker} или произошла ошибка при их обработке\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "259b75fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== АНАЛИЗ НАСТРОЕНИЙ ===\n",
      "GAZP\n",
      "\n",
      "Анализ настроений для тикера GAZP...\n",
      "Результаты анализа настроений сохранены в /Users/aeshef/Documents/GitHub/kursach/data/processed_data/GAZP/news_analysis/GAZP_news_with_sentiment.csv\n",
      "Обработано настроение для 139 новостей\n",
      "Распределение настроений:\n",
      "sentiment_category\n",
      "neutral     132\n",
      "positive      5\n",
      "negative      2\n",
      "Name: count, dtype: int64\n",
      "SBER\n",
      "\n",
      "Анализ настроений для тикера SBER...\n",
      "Результаты анализа настроений сохранены в /Users/aeshef/Documents/GitHub/kursach/data/processed_data/SBER/news_analysis/SBER_news_with_sentiment.csv\n",
      "Обработано настроение для 43 новостей\n",
      "Распределение настроений:\n",
      "sentiment_category\n",
      "neutral     41\n",
      "positive     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== АНАЛИЗ НАСТРОЕНИЙ ===\")\n",
    "sentiment_results = {}\n",
    "\n",
    "for ticker, news_df in processed_news.items():\n",
    "    print(ticker)\n",
    "    print(f\"\\nАнализ настроений для тикера {ticker}...\")\n",
    "    \n",
    "    output_dir = os.path.join(base_dir, 'data', 'processed_data', ticker, 'news_analysis')\n",
    "    \n",
    "    news_with_sentiment = sentiment_analyzer.analyze_ticker_news(\n",
    "        news_df, \n",
    "        save_path=os.path.join(output_dir, f\"{ticker}_news_with_sentiment.csv\")\n",
    "    )\n",
    "    \n",
    "    daily_sentiment = sentiment_analyzer.create_daily_sentiment_series(news_with_sentiment)\n",
    "    daily_sentiment.to_csv(os.path.join(output_dir, f\"{ticker}_daily_sentiment.csv\"), index=False)\n",
    "    \n",
    "    sentiment_results[ticker] = {\n",
    "        'news_with_sentiment': news_with_sentiment,\n",
    "        'daily_sentiment': daily_sentiment\n",
    "    }\n",
    "    \n",
    "    print(f\"Обработано настроение для {len(news_with_sentiment)} новостей\")\n",
    "    \n",
    "    sentiment_counts = news_with_sentiment['sentiment_category'].value_counts()\n",
    "    print(\"Распределение настроений:\")\n",
    "    print(sentiment_counts)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(x='sentiment_category', data=news_with_sentiment)\n",
    "    plt.title(f'Распределение настроений для {ticker}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{ticker}_sentiment_distribution.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e949b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ОБНАРУЖЕНИЕ СОБЫТИЙ ===\n",
      "\n",
      "Обнаружение событий для тикера GAZP...\n",
      "Статистика по типам событий:\n",
      "- earnings: 16 новостей\n",
      "- dividends: 13 новостей\n",
      "- mergers_acquisitions: 0 новостей\n",
      "- regulatory: 38 новостей\n",
      "- management: 3 новостей\n",
      "- products: 5 новостей\n",
      "- macroeconomic: 18 новостей\n",
      "\n",
      "Обнаружение событий для тикера SBER...\n",
      "Статистика по типам событий:\n",
      "- earnings: 8 новостей\n",
      "- dividends: 7 новостей\n",
      "- mergers_acquisitions: 0 новостей\n",
      "- regulatory: 18 новостей\n",
      "- management: 0 новостей\n",
      "- products: 0 новостей\n",
      "- macroeconomic: 7 новостей\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ОБНАРУЖЕНИЕ СОБЫТИЙ ===\")\n",
    "events_results = {}\n",
    "\n",
    "for ticker, results in sentiment_results.items():\n",
    "    print(f\"\\nОбнаружение событий для тикера {ticker}...\")\n",
    "    \n",
    "    news_with_sentiment = results['news_with_sentiment']\n",
    "    \n",
    "    output_dir = os.path.join(base_dir, 'data', 'processed_data', ticker, 'news_analysis')\n",
    "    \n",
    "    news_with_events = event_detector.detect_events(news_with_sentiment)\n",
    "    news_with_events = event_detector.assess_event_impact(news_with_events)\n",
    "    news_with_events.to_csv(os.path.join(output_dir, f\"{ticker}_news_with_events.csv\"), index=False)\n",
    "    \n",
    "    daily_events = event_detector.create_event_time_series(news_with_events)\n",
    "    daily_events.to_csv(os.path.join(output_dir, f\"{ticker}_daily_events.csv\"), index=False)\n",
    "    \n",
    "    events_results[ticker] = {\n",
    "        'news_with_events': news_with_events,\n",
    "        'daily_events': daily_events\n",
    "    }\n",
    "    \n",
    "    event_columns = [col for col in news_with_events.columns if col.startswith('event_') and col not in ['event_impact', 'event_direction', 'has_event']]\n",
    "    \n",
    "    print(\"Статистика по типам событий:\")\n",
    "    for col in event_columns:\n",
    "        event_count = news_with_events[col].sum()\n",
    "        event_name = col.replace('event_', '')\n",
    "        print(f\"- {event_name}: {event_count} новостей\")\n",
    "    \n",
    "    event_counts = {col.replace('event_', ''): news_with_events[col].sum() for col in event_columns}\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(event_counts.keys()), y=list(event_counts.values()))\n",
    "    plt.title(f'Распределение типов событий для {ticker}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{ticker}_event_distribution.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d844a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ИЗВЛЕЧЕНИЕ ПРИЗНАКОВ ДЛЯ МОДЕЛЕЙ ===\n",
      "\n",
      "Создание признаков для тикера GAZP...\n",
      "Выделено 3 тем из новостей\n",
      "Ключевые слова для первой темы: на, по, что, рф, сша, не, gazp, руб, газа, газпром\n",
      "Создано 17 признаков временных рядов\n",
      "\n",
      "Создание признаков для тикера SBER...\n",
      "Выделено 3 тем из новостей\n",
      "Ключевые слова для первой темы: руб, млн, sber, за, по, на, sberp, дельта, подключить, покупки\n",
      "Создано 17 признаков временных рядов\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ИЗВЛЕЧЕНИЕ ПРИЗНАКОВ ДЛЯ МОДЕЛЕЙ ===\")\n",
    "features_results = {}\n",
    "\n",
    "for ticker, events in events_results.items():\n",
    "    print(f\"\\nСоздание признаков для тикера {ticker}...\")\n",
    "    \n",
    "    news_with_events = events['news_with_events']\n",
    "    daily_sentiment = sentiment_results[ticker]['daily_sentiment']\n",
    "    \n",
    "    output_dir = os.path.join(base_dir, 'data', 'processed_data', ticker, 'news_analysis')\n",
    "    \n",
    "    try:\n",
    "        news_with_topics, topic_dict = feature_extractor.create_topic_features(\n",
    "            news_with_events, \n",
    "            text_column='clean_text',\n",
    "            n_topics=3\n",
    "        )\n",
    "        \n",
    "        with open(os.path.join(output_dir, f\"{ticker}_topics.txt\"), 'w') as f:\n",
    "            for topic, keywords in topic_dict.items():\n",
    "                f.write(f\"{topic}: {keywords}\\n\")\n",
    "                \n",
    "        print(f\"Выделено {len(topic_dict)} тем из новостей\")\n",
    "        print(\"Ключевые слова для первой темы:\", topic_dict.get('topic_0', 'Нет данных'))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при извлечении тем: {e}\")\n",
    "        news_with_topics = news_with_events\n",
    "        topic_dict = {}\n",
    "    \n",
    "    sentiment_features = feature_extractor.create_time_series_features(daily_sentiment)\n",
    "    sentiment_features.to_csv(os.path.join(output_dir, f\"{ticker}_sentiment_features.csv\"), index=False)\n",
    "    \n",
    "    features_results[ticker] = {\n",
    "        'news_with_topics': news_with_topics,\n",
    "        'topic_dict': topic_dict,\n",
    "        'sentiment_features': sentiment_features\n",
    "    }\n",
    "    \n",
    "    print(f\"Создано {len(sentiment_features.columns)} признаков временных рядов\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sentiment_features['date'], sentiment_features['avg_sentiment'], label='Среднее настроение')\n",
    "    plt.plot(sentiment_features['date'], sentiment_features['sentiment_ma_7d'], label='Скользящее среднее (7 дней)')\n",
    "    plt.title(f'Динамика настроений для {ticker}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sentiment_features['date'], sentiment_features['news_count'], label='Количество новостей')\n",
    "    plt.title(f'Объем новостей для {ticker}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{ticker}_sentiment_dynamics.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a56242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ИНТЕГРАЦИЯ С ЦЕНОВЫМИ ДАННЫМИ ===\n",
      "\n",
      "Интеграция новостей с ценами для тикера GAZP...\n",
      "Найден файл с ценами: GAZP_2025-01-02_2025-04-09.parquet\n",
      "Структура данных из parquet файла: ['date', 'open', 'close', 'min', 'max', 'volume']\n",
      "Пример агрегированных дневных данных:\n",
      "        date    open   close    high     low   volume\n",
      "0 2025-01-02  130.05  131.39  132.02  130.70   215415\n",
      "1 2025-01-03  131.94  127.98  132.90  127.49  4534358\n",
      "2 2025-01-04  127.98  128.39  128.59  127.70    64800\n",
      "Структура объединенных данных: (98, 21), колонки: ['open', 'close', 'high', 'low', 'volume']...\n",
      "Структура данных ML: (98, 190)\n",
      "Временной период: с 2025-01-02 00:00:00 по 2025-04-09 00:00:00\n",
      "Создан график сравнения цен и настроений\n",
      "\n",
      "Интеграция новостей с ценами для тикера SBER...\n",
      "Найден файл с ценами: SBER_2025-01-02_2025-04-09.parquet\n",
      "Структура данных из parquet файла: ['date', 'open', 'close', 'min', 'max', 'volume']\n",
      "Пример агрегированных дневных данных:\n",
      "        date    open   close    high     low   volume\n",
      "0 2025-01-02  281.67  280.40  282.46  279.81    52231\n",
      "1 2025-01-03  279.37  272.54  282.35  271.88  4318069\n",
      "2 2025-01-04  272.47  272.15  272.70  271.60    36017\n",
      "Структура объединенных данных: (98, 21), колонки: ['open', 'close', 'high', 'low', 'volume']...\n",
      "Структура данных ML: (98, 190)\n",
      "Временной период: с 2025-01-02 00:00:00 по 2025-04-09 00:00:00\n",
      "Создан график сравнения цен и настроений\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ИНТЕГРАЦИЯ С ЦЕНОВЫМИ ДАННЫМИ ===\")\n",
    "ml_features_results = {}\n",
    "\n",
    "integrator = NewsIntegration()\n",
    "\n",
    "for ticker, features in features_results.items():\n",
    "    print(f\"\\nИнтеграция новостей с ценами для тикера {ticker}...\")\n",
    "    \n",
    "    sentiment_features = features['sentiment_features']\n",
    "\n",
    "    output_dir = os.path.join(base_dir, 'data', 'processed_data', ticker, 'news_analysis')\n",
    "    ticker_dir = os.path.join(base_dir, 'data', 'processed_data', ticker)\n",
    "    parquet_files = [f for f in os.listdir(ticker_dir) if f.endswith('.parquet') and ticker in f]\n",
    "    \n",
    "    if not parquet_files:\n",
    "        print(f\"Ценовые данные в формате parquet для {ticker} не найдены\")\n",
    "        continue\n",
    "    \n",
    "    parquet_file = sorted(parquet_files, key=lambda x: os.path.getmtime(os.path.join(ticker_dir, x)), reverse=True)[0]\n",
    "    price_path = os.path.join(ticker_dir, parquet_file)\n",
    "    \n",
    "    print(f\"Найден файл с ценами: {parquet_file}\")\n",
    "    \n",
    "    try:\n",
    "        import pandas as pd\n",
    "        \n",
    "\n",
    "        price_df = pd.read_parquet(price_path)\n",
    "        \n",
    "        print(f\"Структура данных из parquet файла: {price_df.columns.tolist()}\")\n",
    "        \n",
    "        price_df['date'] = pd.to_datetime(price_df['date'])\n",
    "        \n",
    "        column_mapping = {'min': 'low', 'max': 'high'}\n",
    "        price_df = price_df.rename(columns=column_mapping)\n",
    "        \n",
    "        price_df['day'] = price_df['date'].dt.date\n",
    "        daily_price = price_df.groupby('day').agg({\n",
    "            'open': 'first',\n",
    "            'close': 'last',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'volume': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        daily_price = daily_price.rename(columns={'day': 'date'})\n",
    "        \n",
    "        daily_price['date'] = pd.to_datetime(daily_price['date'])\n",
    "        \n",
    "        print(\"Пример агрегированных дневных данных:\")\n",
    "        print(daily_price.head(3))\n",
    "        \n",
    "        combined_df = integrator.merge_news_with_price_data(\n",
    "            sentiment_features,\n",
    "            daily_price,\n",
    "            date_column='date'\n",
    "        )\n",
    "        \n",
    "        print(f\"Структура объединенных данных: {combined_df.shape}, колонки: {combined_df.columns.tolist()[:5]}...\")\n",
    "        \n",
    "        try:\n",
    "            ml_df = combined_df.copy()\n",
    "            \n",
    "            prediction_horizon = 5\n",
    "            ml_df['target_return'] = ml_df['close'].pct_change(prediction_horizon).shift(-prediction_horizon)\n",
    "            \n",
    "            news_features = [col for col in ml_df.columns if col.startswith(('sentiment', 'news_count', 'event_', 'topic_'))]\n",
    "            \n",
    "            new_columns = {}\n",
    "            \n",
    "            for feature in news_features:\n",
    "                for lag in [1, 2, 3, 5, 10]:\n",
    "                    new_columns[f'{feature}_lag{lag}'] = ml_df[feature].shift(lag)\n",
    "            \n",
    "            for feature in news_features:\n",
    "                for window in [3, 7, 14, 30]:\n",
    "                    new_columns[f'{feature}_ma{window}'] = ml_df[feature].rolling(window=window, min_periods=1).mean()\n",
    "                \n",
    "                new_columns[f'{feature}_std7'] = ml_df[feature].rolling(window=7, min_periods=1).std()\n",
    "                new_columns[f'{feature}_std14'] = ml_df[feature].rolling(window=14, min_periods=1).std()\n",
    "\n",
    "            if 'avg_sentiment' in ml_df.columns:\n",
    "                new_columns['sentiment_return_corr'] = ml_df['avg_sentiment'].rolling(window=14).corr(ml_df['close'].pct_change())\n",
    "            \n",
    "            if 'news_count' in ml_df.columns:\n",
    "                new_columns['news_count_volatility'] = ml_df['news_count'] * ml_df['close'].pct_change().rolling(window=7).std()\n",
    "            \n",
    "            ml_df = pd.concat([ml_df, pd.DataFrame(new_columns)], axis=1)\n",
    "            \n",
    "            ml_df = ml_df.reset_index()\n",
    "            ml_features = ml_df\n",
    "            \n",
    "            print(f\"Структура данных ML: {ml_features.shape}\")\n",
    "            if 'date' in ml_features.columns:\n",
    "                print(f\"Временной период: с {ml_features['date'].min()} по {ml_features['date'].max()}\")\n",
    "            else:\n",
    "                print(f\"Колонки в ml_features: {ml_features.columns.tolist()[:5]}...\")\n",
    "            \n",
    "            ml_features.to_csv(os.path.join(output_dir, f\"{ticker}_ml_features.csv\"))\n",
    "            ml_features_results[ticker] = ml_features\n",
    "            \n",
    "            try:\n",
    "                date_col = 'date' if 'date' in ml_features.columns else 'index'\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                ax1 = plt.gca()\n",
    "                ax2 = ax1.twinx()\n",
    "                \n",
    "                ax1.plot(ml_features[date_col], ml_features['close'], 'b-', label='Цена')\n",
    "                ax2.plot(ml_features[date_col], ml_features['avg_sentiment'], 'r-', label='Настроение')\n",
    "                \n",
    "                ax1.set_xlabel('Дата')\n",
    "                ax1.set_ylabel('Цена закрытия', color='b')\n",
    "                ax2.set_ylabel('Среднее настроение', color='r')\n",
    "                \n",
    "                lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "                lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "                ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "                \n",
    "                plt.title(f'Сравнение цен и настроений для {ticker}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, f\"{ticker}_price_vs_sentiment.png\"))\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"Создан график сравнения цен и настроений\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при создании визуализации: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при создании признаков ML: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при интеграции данных: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3fe0ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== СОЗДАНИЕ СВОДНОГО ОТЧЕТА ===\n",
      "Сводный отчет сохранен в /Users/aeshef/Documents/GitHub/kursach/data/news_analysis_summary.txt\n",
      "\n",
      "=== АНАЛИЗ НОВОСТЕЙ ЗАВЕРШЕН ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== СОЗДАНИЕ СВОДНОГО ОТЧЕТА ===\")\n",
    "\n",
    "summary_file = os.path.join(base_dir, 'data', 'news_analysis_summary.txt')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"=== СВОДНЫЙ ОТЧЕТ ПО АНАЛИЗУ НОВОСТЕЙ ===\\n\\n\")\n",
    "    f.write(f\"Дата анализа: {datetime.datetime.now()}\\n\\n\")\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        f.write(f\"Тикер: {ticker}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        if ticker in processed_news:\n",
    "            news_count = len(processed_news[ticker])\n",
    "            f.write(f\"Количество новостей: {news_count}\\n\")\n",
    "        else:\n",
    "            f.write(\"Новости не обработаны\\n\")\n",
    "        \n",
    "        if ticker in sentiment_results:\n",
    "            sentiment_df = sentiment_results[ticker]['news_with_sentiment']\n",
    "            sentiment_stats = sentiment_df['sentiment_category'].value_counts()\n",
    "            f.write(\"Распределение настроений:\\n\")\n",
    "            for category, count in sentiment_stats.items():\n",
    "                f.write(f\"- {category}: {count} ({count/len(sentiment_df)*100:.1f}%)\\n\")\n",
    "        \n",
    "        if ticker in events_results:\n",
    "            events_df = events_results[ticker]['news_with_events']\n",
    "            event_columns = [col for col in events_df.columns if col.startswith('event_') and col not in ['event_impact', 'event_direction', 'has_event']]\n",
    "            \n",
    "            f.write(\"Статистика по типам событий:\\n\")\n",
    "            for col in event_columns:\n",
    "                event_count = events_df[col].sum()\n",
    "                event_name = col.replace('event_', '')\n",
    "                f.write(f\"- {event_name}: {event_count} новостей\\n\")\n",
    "        \n",
    "        if ticker in ml_features_results:\n",
    "            ml_df = ml_features_results[ticker]\n",
    "            f.write(f\"Создан набор данных для ML моделей: {len(ml_df)} записей, {len(ml_df.columns)} признаков\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Сводный отчет сохранен в {summary_file}\")\n",
    "\n",
    "print(\"\\n=== АНАЛИЗ НОВОСТЕЙ ЗАВЕРШЕН ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65eb6eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ПРЕДОБРАБОТКА НОВОСТЕЙ ===\n",
      "\n",
      "Предобработка новостей для тикера SBER...\n",
      "/Users/aeshef/Documents/GitHub/kursach/data/processed_data/SBER/news_analysis\n",
      "Объединенные данные сохранены в /Users/aeshef/Documents/GitHub/kursach/data/processed_data/SBER/SBER_all_news_processed.csv\n",
      "Обработано 43 новостей для SBER\n",
      "Пример обработанной новости:\n",
      "Дата: 2025-01-01 00:00:00\n",
      "Текст до: #SBER\n",
      "**Правительству РФ и Сберу поручено обеспечить сотрудничество с Китаем при проведении исследов...\n",
      "Текст после: SBER Правительству РФ и Сберу поручено обеспечить сотрудничество с Китаем при проведении исследований в сфере ИИ Интерфакс[:100]...\n",
      "\n",
      "=== АНАЛИЗ НАСТРОЕНИЙ ===\n",
      "SBER\n",
      "\n",
      "Анализ настроений для тикера SBER...\n",
      "Результаты анализа настроений сохранены в /Users/aeshef/Documents/GitHub/kursach/data/processed_data/SBER/news_analysis/SBER_news_with_sentiment.csv\n",
      "Обработано настроение для 43 новостей\n",
      "Распределение настроений:\n",
      "sentiment_category\n",
      "neutral     41\n",
      "positive     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ОБНАРУЖЕНИЕ СОБЫТИЙ ===\n",
      "\n",
      "Обнаружение событий для тикера SBER...\n",
      "Статистика по типам событий:\n",
      "- earnings: 8 новостей\n",
      "- dividends: 7 новостей\n",
      "- mergers_acquisitions: 0 новостей\n",
      "- regulatory: 18 новостей\n",
      "- management: 0 новостей\n",
      "- products: 0 новостей\n",
      "- macroeconomic: 7 новостей\n",
      "\n",
      "=== ИЗВЛЕЧЕНИЕ ПРИЗНАКОВ ДЛЯ МОДЕЛЕЙ ===\n",
      "\n",
      "Создание признаков для тикера SBER...\n",
      "Выделено 3 тем из новостей\n",
      "Ключевые слова для первой темы: руб, млн, sber, за, по, на, sberp, дельта, подключить, покупки\n",
      "Создано 17 признаков временных рядов\n",
      "\n",
      "=== ИНТЕГРАЦИЯ С ЦЕНОВЫМИ ДАННЫМИ ===\n",
      "\n",
      "Интеграция новостей с ценами для тикера SBER...\n",
      "Найден файл с ценами: SBER_2025-01-02_2025-04-09.parquet\n",
      "Структура данных из parquet файла: ['date', 'open', 'close', 'min', 'max', 'volume']\n",
      "Пример агрегированных дневных данных:\n",
      "        date    open   close    high     low   volume\n",
      "0 2025-01-02  281.67  280.40  282.46  279.81    52231\n",
      "1 2025-01-03  279.37  272.54  282.35  271.88  4318069\n",
      "2 2025-01-04  272.47  272.15  272.70  271.60    36017\n",
      "Структура объединенных данных: (98, 21), колонки: ['open', 'close', 'high', 'low', 'volume']...\n",
      "Структура данных ML: (98, 190)\n",
      "Временной период: с 2025-01-02 00:00:00 по 2025-04-09 00:00:00\n",
      "Создан график сравнения цен и настроений\n",
      "\n",
      "=== СОЗДАНИЕ СВОДНОГО ОТЧЕТА ===\n",
      "Сводный отчет сохранен в /Users/aeshef/Documents/GitHub/kursach/data/news_analysis_summary.txt\n",
      "\n",
      "=== АНАЛИЗ НОВОСТЕЙ ЗАВЕРШЕН ===\n"
     ]
    }
   ],
   "source": [
    "import news_pipeline\n",
    "\n",
    "importlib.reload(news_pipeline)\n",
    "from news_pipeline import NewsPipeline\n",
    "\n",
    "np = NewsPipeline()\n",
    "np.run_pipeline(\n",
    "    tickers = [\"SBER\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273971d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
